<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
  <title>æ‰‹åŠ¿è¯†åˆ« + éª¨æž¶ç»˜åˆ¶</title>
  <style>
    body { margin: 0; background: #111; color: #fff; text-align: center; font-family: sans-serif; }
    #gesture { font-size: 24px; margin: 10px; color: #0f0; }
    video, canvas { width: 100%; max-width: 480px; border-radius: 12px; }
  </style>
</head>
<body>
  <h2>ðŸ“± iPhone Safari å…¼å®¹æ‰‹åŠ¿è¯†åˆ«</h2>
  <div id="gesture">è¯†åˆ«ä¸­...</div>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>

  <!-- å¼•å…¥ä¾èµ– -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const gestureEl = document.getElementById('gesture');

    // åˆå§‹åŒ– MediaPipe Hands
    const hands = new Hands({
      locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
    });
    hands.setOptions({
      maxNumHands: 1,
      modelComplexity: 1,
      minDetectionConfidence: 0.7,
      minTrackingConfidence: 0.6
    });

    hands.onResults(onResults);

    // å¯åŠ¨æ‘„åƒå¤´
    async function startCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: 'user', width: 640, height: 480 }
      });
      video.srcObject = stream;

      video.onloadedmetadata = () => {
        video.play();
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        runDetection();
      };
    }

    async function runDetection() {
      await hands.send({ image: video });
      requestAnimationFrame(runDetection);
    }

    // å®šä¹‰å¸¸ç”¨æ‰‹åŠ¿
    const GE = new fp.GestureEstimator([
      fp.Gestures.ThumbsUpGesture,
      fp.Gestures.VictoryGesture,
      fp.Gestures.OpenPalmGesture,
      fp.Gestures.FistGesture
    ]);

    // ç»˜åˆ¶æ‰‹éª¨æž¶
    function drawLandmarks(landmarks) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      drawConnectors(ctx, landmarks, HAND_CONNECTIONS, { color: '#00FF00', lineWidth: 3 });
      drawLandmarksFn(ctx, landmarks, { color: '#FF0000', lineWidth: 2 });
    }

    function onResults(results) {
      if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
        const landmarks = results.multiHandLandmarks[0];

        // ç»˜åˆ¶éª¨æž¶
        drawLandmarks(landmarks);

        // è§£æž landmarks ä¸º fingerpose æ ¼å¼
        const gesture = GE.estimate(landmarks, 7.5);
        if (gesture.gestures.length > 0) {
          const best = gesture.gestures.reduce((p, c) => p.score > c.score ? p : c);
          gestureEl.innerText = `è¯†åˆ«åˆ°æ‰‹åŠ¿ï¼š${best.name}`;
        } else {
          gestureEl.innerText = 'æ‰‹åŠ¿æœªçŸ¥';
        }
      } else {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        gestureEl.innerText = 'è¯†åˆ«ä¸­...';
      }
    }

    startCamera();
  </script>
</body>
</html>
